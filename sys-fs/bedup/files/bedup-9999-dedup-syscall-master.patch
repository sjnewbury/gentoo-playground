diff --git a/.gitmodules b/.gitmodules
index 7b0b171..a647c9c 100644
--- a/.gitmodules
+++ b/.gitmodules
@@ -1,3 +1,6 @@
 [submodule "btrfs"]
 	path = btrfs
 	url = https://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-progs.git
+[submodule "duperemove"]
+	path = duperemove
+	url = https://github.com/markfasheh/duperemove.git
diff --git a/MANIFEST.in b/MANIFEST.in
index 83af674..6cd42b1 100644
--- a/MANIFEST.in
+++ b/MANIFEST.in
@@ -8,6 +8,7 @@ include btrfs/radix-tree.h
 include btrfs/extent-cache.h
 include btrfs/rbtree.h
 include btrfs/extent_io.h
+include duperemove/btrfs-ioctl.h
 
 include COPYING
 include README.rst
diff --git a/bedup/filesystem.py b/bedup/filesystem.py
index bd0c09b..0cd3f5c 100644
--- a/bedup/filesystem.py
+++ b/bedup/filesystem.py
@@ -539,7 +539,7 @@ class WholeFS(object):
         assert self._vol_map
         frozen_skipped = 0
         for vol in lo:
-            if vol.root_info.is_frozen:
+            if vol.root_info.is_frozen and False:
                 vol.close()
                 frozen_skipped += 1
             else:
@@ -584,7 +584,7 @@ class WholeFS(object):
                 for vol in lo:
                     if vol in loaded:
                         continue
-                    if vol.root_info.is_frozen and vol not in sta:
+                    if vol.root_info.is_frozen and vol not in sta and False:
                         vol.close()
                         skipped += 1
                     else:
diff --git a/bedup/platform/btrfs.py b/bedup/platform/btrfs.py
index 0ced353..9f891da 100644
--- a/bedup/platform/btrfs.py
+++ b/bedup/platform/btrfs.py
@@ -1,5 +1,4 @@
 # vim: set fileencoding=utf-8 sw=4 ts=4 et :
-
 # bedup - Btrfs deduplication
 # Copyright (C) 2015 Gabriel de Perthuis <g2p.code+bedup@gmail.com>
 #
@@ -130,6 +129,36 @@ struct btrfs_ioctl_ino_lookup_args {
 };
 
 
+/* duperemove/btrfs-ioctl.h */
+
+#define BTRFS_IOC_FILE_EXTENT_SAME ...
+#define BTRFS_SAME_DATA_DIFFERS ...
+
+struct btrfs_ioctl_same_extent_info {
+	int64_t fd;			/* in - destination file */
+	uint64_t logical_offset;	/* in - start of extent in destination */
+	uint64_t bytes_deduped;		/* out - total # of bytes we
+					 * were able to dedupe from
+					 * this file */
+	/* status of this dedupe operation:
+	 * 0 if dedup succeeds
+	 * < 0 for error
+	 * == BTRFS_SAME_DATA_DIFFERS if data differs
+	 */
+	int32_t status;			/* out - see above description */
+	uint32_t reserved;
+};
+
+struct btrfs_ioctl_same_args {
+	uint64_t logical_offset;	/* in - start of extent in source */
+	uint64_t length;		/* in - length of extent */
+	uint16_t total_files;		/* in - total elements in info array */
+	uint16_t files_deduped;		/* out - number of files that got deduped */
+	uint32_t reserved;
+	struct btrfs_ioctl_same_extent_info info[0];
+};
+
+
 /* ctree.h */
 
 #define BTRFS_EXTENT_DATA_KEY ...
@@ -327,11 +356,13 @@ uint64_t btrfs_root_generation(struct btrfs_root_item *s);
 lib = cffi_support.verify(ffi, '''
     #include <btrfs/ioctl.h>
     #include <btrfs/ctree.h>
+    #include <duperemove/btrfs-ioctl.h>
     ''',
     include_dirs=[cffi_support.BTRFS_INCLUDE_DIR])
 
 
 BTRFS_FIRST_FREE_OBJECTID = lib.BTRFS_FIRST_FREE_OBJECTID
+BTRFS_SAME_DATA_DIFFERS = lib.BTRFS_SAME_DATA_DIFFERS
 
 u64_max = ffi.cast('uint64_t', -1)
 
@@ -603,6 +634,61 @@ def clone_data(dest, src, check_first):
     return True
 
 
+# patch v2 does 16M at once;
+# let's not make that part of the abi though
+# 1M should be safe
+MAX_DEDUP_LEN = 1 * 2**20
+MAX_DEDUP_DESTS = ((4096 - ffi.sizeof('struct btrfs_ioctl_same_args'))
+    // ffi.sizeof('struct btrfs_ioctl_same_extent_info'))
+
+
+def ranges_same(length, src, dests):
+    dest_count = len(dests)
+    assert dest_count
+    assert length > 0
+    return sum((
+        _ranges_same(
+            length, src, dests[i*MAX_DEDUP_DESTS:(i+1)*MAX_DEDUP_DESTS])
+        for i in range((dest_count - 1) // MAX_DEDUP_DESTS + 1)), [])
+
+
+def _ranges_same(length, src, dests):
+    dest_count = len(dests)
+    statuses = [set() for i in range(dest_count)]
+    dedup_lens = [0 for i in range(dest_count)]
+
+    alloc_size = (
+        ffi.sizeof('struct btrfs_ioctl_same_args')
+        + dest_count * ffi.sizeof('struct btrfs_ioctl_same_extent_info'))
+
+    if alloc_size == 1024:
+        alloc_size += 1
+
+    if alloc_size > 4096:
+        raise ValueError('Argument larger than 4k, the kernel won\'t handle it')
+
+    # keep a reference around; the cast struct won't have ownership
+    args_cbuf = ffi.new('char[]', alloc_size)
+    args = ffi.cast('struct btrfs_ioctl_same_args *', args_cbuf)
+    # Misnamed, think total_dest_files
+    args.total_files = dest_count
+    src_fd, args.logical_offset = src
+
+    while length >= 4096:
+        args.length = len_i = min(length, MAX_DEDUP_LEN) // 4096 * 4096
+        length -= len_i
+        for i in range(dest_count):
+            args.info[i].fd, args.info[i].logical_offset = fd, offset = dests[i]
+            dests[i] = (fd, offset + len_i)
+        ioctl_pybug(src_fd, lib.BTRFS_IOC_FILE_EXTENT_SAME, ffi.buffer(args_cbuf))
+        args.logical_offset += len_i
+        for i in range(dest_count):
+            statuses[i].add(args.info[i].status)
+            dedup_lens[i] += args.info[i].bytes_deduped
+
+    return list(zip(statuses, dedup_lens))
+
+
 def defragment(fd):
     # XXX Can remove compression as a side-effect
     # Also, can unshare extents.
diff --git a/bedup/tracking.py b/bedup/tracking.py
index 1f1f1b2..0514ac0 100644
--- a/bedup/tracking.py
+++ b/bedup/tracking.py
@@ -34,7 +34,8 @@ from sqlalchemy.sql import and_, select, func, literal_column
 from uuid import UUID
 
 from .platform.btrfs import (
-    get_root_generation, clone_data, defragment as btrfs_defragment, lib)
+    get_root_generation, clone_data, ranges_same,
+    defragment as btrfs_defragment, BTRFS_SAME_DATA_DIFFERS, lib)
 from .platform.openat import fopenat, fopenat_rw
 
 from .datetime import system_now
@@ -533,7 +534,7 @@ def dedup_tracked1(ds, comm1):
                     continue
                 raise
             try:
-                afile = fopenat_rw(inode.vol.live.fd, path)
+                afile = fopenat(inode.vol.live.fd, path)
             except IOError as e:
                 if e.errno == errno.ETXTBSY:
                     # The file contains the image of a running process,
@@ -563,26 +564,28 @@ def dedup_tracked1(ds, comm1):
             for afile in files:
                 stack.enter_context(closing(afile))
             # Enter this context last
-            immutability = stack.enter_context(ImmutableFDs(fds))
+            #immutability = stack.enter_context(ImmutableFDs(fds))
 
             # With a false positive, some kind of cmp pass that compares
             # all files at once might be more efficient that hashing.
             for afile in files:
                 fd = afile.fileno()
                 inode = fd_inodes[fd]
-                if fd in immutability.fds_in_write_use:
+                if False and fd in immutability.fds_in_write_use:
                     ds.tt.notify('File %r is in use, skipping' % fd_names[fd])
                     ds.skip(inode)
                     continue
-                hasher = hashlib.sha1()
-                try:
-                    for buf in iter(lambda: afile.read(BUFSIZE), b''):
-                        hasher.update(buf)
-                except OSError as e:
-                    if e.errno == errno.EIO:
-                        continue
-                    raise
-
+                if False:
+                    hasher = hashlib.sha1()
+                    try:
+                        for buf in iter(lambda: afile.read(BUFSIZE), b''):
+                            hasher.update(buf)
+                    except IOError as e:
+                        if e.errno == errno.EIO:
+                            continue
+                        raise
+                else:
+                    afile.seek(0, os.SEEK_END)
                 # Gets rid of a race condition
                 st = os.fstat(fd)
                 if st.st_ino != inode.ino:
@@ -602,7 +605,10 @@ def dedup_tracked1(ds, comm1):
                         ds.skip(inode)
                     continue
 
-                by_hash[hasher.digest()].append(afile)
+                if False:
+                    by_hash[hasher.digest()].append(afile)
+                else:
+                    by_hash[None].append(afile)
                 ds.tt.update(fhash=None)
 
             for fileset in by_hash.values():
@@ -619,39 +625,58 @@ def dedup_fileset(ds, fileset, fd_names, fd_inodes, size):
         btrfs_defragment(sfd)
     dfiles = fileset[1:]
     dfiles_successful = []
-    for dfile in dfiles:
-        dfd = dfile.fileno()
-        ddesc = fd_inodes[dfd].vol.live.describe_path(
-            fd_names[dfd])
-        if not cmp_files(sfile, dfile):
-            # Probably a bug since we just used a crypto hash
-            ds.tt.notify('Files differ: %r %r' % (sdesc, ddesc))
-            assert False, (sdesc, ddesc)
-            return
-        try:
-            deduped = clone_data(dest=dfd, src=sfd, check_first=True)
-        except IOError as e:
-            if e.errno == errno.EINVAL:
-                ds.tt.notify(
-                    'Error deduplicating, maybe a file is marked NODATACOW:\n'
-                    '- %r\n- %r' % (sdesc, ddesc))
+    if False:
+        for dfile in dfiles:
+            dfd = dfile.fileno()
+            ddesc = fd_inodes[dfd].vol.live.describe_path(
+                fd_names[dfd])
+            if not cmp_files(sfile, dfile):
+                # Probably a bug since we just used a crypto hash
+                ds.tt.notify('Files differ: %r %r' % (sdesc, ddesc))
+                assert False, (sdesc, ddesc)
                 return
-            raise
-        if deduped:
-            ds.tt.notify(
-                'Deduplicated:\n- %r\n- %r' % (sdesc, ddesc))
-            dfiles_successful.append(dfile)
-            ds.space_gain += size
+            try:
+                deduped = clone_data(dest=dfd, src=sfd, check_first=True)
+            except IOError as e:
+                if e.errno == errno.EINVAL:
+                    ds.tt.notify(
+                        'Error deduplicating, maybe a file is marked NODATACOW:\n'
+                        '- %r\n- %r' % (sdesc, ddesc))
+                    return
+                raise
+            if deduped:
+                ds.tt.notify(
+                    'Deduplicated:\n- %r\n- %r' % (sdesc, ddesc))
+                dfiles_successful.append(dfile)
+                ds.space_gain += size
+                ds.tt.update(space_gain=ds.space_gain)
+            elif False:
+                # Often happens when there are multiple files with
+                # the same extents, plus one with the same size and
+                # mini-hash but a difference elsewhere.
+                # We hash the same extents multiple times, but
+                # I assume the data is shared in the vfs cache.
+                ds.tt.notify(
+                    'Did not deduplicate (same extents): %r %r' % (
+                        sdesc, ddesc))
+    else:
+        dinfos = ranges_same(
+            size, (sfd, 0), [(dfile.fileno(), 0) for dfile in dfiles])
+        ds.tt.notify('Deduplicated:\n- %r' % (sdesc, ))
+        for (dfile, (statuses, bytes_deduped)) in zip(dfiles, dinfos):
+            if 0 in statuses:
+                dfiles_successful.append(dfile)
+                statuses.remove(0)
+            dfd = dfile.fileno()
+            ddesc = fd_inodes[dfd].vol.live.describe_path(fd_names[dfd])
+            if BTRFS_SAME_DATA_DIFFERS in statuses:
+                ddesc += ' (differences)'
+                statuses.remove(BTRFS_SAME_DATA_DIFFERS)
+            if statuses:
+                ddesc += ' (errors: %s)' % (statuses, )
+            ds.tt.notify('- %r' % (ddesc,))
+            ds.space_gain += bytes_deduped
             ds.tt.update(space_gain=ds.space_gain)
-        elif False:
-            # Often happens when there are multiple files with
-            # the same extents, plus one with the same size and
-            # mini-hash but a difference elsewhere.
-            # We hash the same extents multiple times, but
-            # I assume the data is shared in the vfs cache.
-            ds.tt.notify(
-                'Did not deduplicate (same extents): %r %r' % (
-                    sdesc, ddesc))
     if dfiles_successful:
         evt = DedupEvent(
             fs=ds.fs.impl, item_size=size, created=system_now())
diff --git a/duperemove b/duperemove
new file mode 160000
index 0000000..4b2d8b7
--- /dev/null
+++ b/duperemove
@@ -0,0 +1 @@
+Subproject commit 4b2d8b74618cc7d56c302adb8116b42bc6a3c53a
